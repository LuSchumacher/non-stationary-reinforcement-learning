{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from scipy.stats import norm, halfnorm, uniform\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "# Get rid of annoying tf warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import bayesflow as bf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from model import generative_model\n",
    "from configurator import configure_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Suppress scientific notation for floats\n",
    "np.set_printoptions(suppress=True)\n",
    "# Configure rng\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NETWORK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA_NAMES = (\"Learning rate\", \"Memory contribution\")\n",
    "THETA_LABELS= (r\"$\\alpha$\", r\"$p$\")\n",
    "ETA_NAMES = (\"Transition scale\", \"Transition scale\")\n",
    "ETA_LABELS= (r\"$\\sigma_{\\alpha}$\", r\"$\\sigma_{p}$\")\n",
    "KAPPA_NAMES = (\"Memory decay\", \"Memory capacity\")\n",
    "KAPPA_LABELS= (r\"$\\phi$\", r\"$c$\")\n",
    "\n",
    "THETA_PRIOR_MEAN = np.array([0.5, 0.5])\n",
    "THETA_PRIOR_STD = np.array([0.3, 0.3])\n",
    "ETA_PRIOR_MEAN = np.round(halfnorm(0, 0.05).mean(), decimals=2)\n",
    "ETA_PRIOR_STD = np.round(halfnorm(0, 0.05).std(), decimals=2)\n",
    "KAPPA_PRIOR_MEAN = np.array([0.5, 4.7])\n",
    "KAPPA_PRIOR_STD = np.array([0.3, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha \\rightarrow$ Learning rate [0, 1] dynamic\n",
    "\n",
    "$\\tau \\rightarrow$ Inverse temperature [0, ] fix it at 10\n",
    "\n",
    "$\\phi \\rightarrow$ Memory decay [0, 1] static\n",
    "\n",
    "$w \\rightarrow$ Memory contribution = $p*min(1, \\frac{C}{n_S})$\n",
    "\n",
    "$p \\rightarrow$ Initial memory weighting [0, 1] dynamic\n",
    "\n",
    "$C \\rightarrow$ Memory capacity [0, ] is usually 5-9 static\n",
    "\n",
    "$n_S \\rightarrow$ Set size in current block\n",
    "\n",
    "$\\gamma \\rightarrow$ Perseveration [0, 1] we don't need this at all because alpha is already dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 1: Stimulus [0, 5]\n",
    "\n",
    "Column 2: Correct response [0, 2]\n",
    "\n",
    "Column 3: Block id [1, 14]\n",
    "\n",
    "Columns 4: Set size [3, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forward_dict = generative_model(32)\n",
    "_ = configure_input(forward_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator_settings = {\n",
    "    \"lstm1_hidden_units\": 512,\n",
    "    \"lstm2_hidden_units\": 256,\n",
    "    \"lstm3_hidden_units\": 128,\n",
    "    \"trainer\": {\n",
    "        \"max_to_keep\": 1,\n",
    "        \"default_lr\": 5e-4,\n",
    "        \"memory\": False,\n",
    "    },\n",
    "    \"local_amortizer_settings\": {\n",
    "        \"num_coupling_layers\": 8,\n",
    "        \"coupling_design\": 'interleaved'\n",
    "    },\n",
    "    \"global_amortizer_settings\": {\n",
    "        \"num_coupling_layers\": 6,\n",
    "        \"coupling_design\": 'interleaved'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_network = bf.networks.HierarchicalNetwork(\n",
    "    [\n",
    "        Sequential(\n",
    "            [\n",
    "                Bidirectional(LSTM(approximator_settings[\"lstm1_hidden_units\"], return_sequences=True)),\n",
    "                Bidirectional(LSTM(approximator_settings[\"lstm2_hidden_units\"], return_sequences=True)),\n",
    "            ]\n",
    "        ),\n",
    "        Sequential(\n",
    "            [\n",
    "                Bidirectional(LSTM(approximator_settings[\"lstm3_hidden_units\"]))\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_network = bf.amortizers.AmortizedPosterior(\n",
    "    bf.networks.InvertibleNetwork(\n",
    "        num_params=2,\n",
    "        **approximator_settings.get(\"local_amortizer_settings\")\n",
    "    )\n",
    ")\n",
    "global_network = bf.amortizers.AmortizedPosterior(\n",
    "    bf.networks.InvertibleNetwork(\n",
    "        num_params=2+2,\n",
    "        **approximator_settings.get(\"global_amortizer_settings\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amortizer = bf.amortizers.TwoLevelAmortizedPosterior(\n",
    "    local_amortizer=local_network,\n",
    "    global_amortizer=global_network,\n",
    "    summary_net=summary_network\n",
    ")\n",
    "trainer = bf.trainers.Trainer(\n",
    "    amortizer=amortizer,\n",
    "    generative_model=generative_model,\n",
    "    configurator=configure_input,\n",
    "    **approximator_settings.get(\"trainer\"),\n",
    "    checkpoint_path=\"../checkpoints/ns_rlwm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if TRAIN_NETWORK:\n",
    "    history = trainer.train_online(\n",
    "        epochs=75, \n",
    "        iterations_per_epoch=1000, \n",
    "        batch_size=16\n",
    "    )\n",
    "else:\n",
    "    history = trainer.loss_history.get_plottable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_losses(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
